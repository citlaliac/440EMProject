{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainurl = 'https://www.sec.gov/Archives/edgar/full-index/'\n",
    "r=requests.get(mainurl)\n",
    "data=r.text\n",
    "soup = BeautifulSoup(data, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []\n",
    "i = 1\n",
    "while i < 200:\n",
    "    years.append((str(i+1993)) + '/')\n",
    "    i += 1\n",
    "    \n",
    "yearHyperlinks = []\n",
    "newUrls = []\n",
    "\n",
    "for i in soup.find_all('a'):\n",
    "    if (i.text + '/') in years:\n",
    "        yearHyperlinks.append(i)\n",
    "\n",
    "for i in yearHyperlinks:\n",
    "    newUrls.append(mainurl + str(i.get('href')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quarters = ['QTR1','QTR2','QTR3','QTR4']\n",
    "newNewUrls = []\n",
    "\n",
    "for url in newUrls:\n",
    "    r=requests.get(url)\n",
    "    data=r.text\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    for tag in soup.find_all('a'):\n",
    "        if tag.text in quarters:\n",
    "            newNewUrls.append(url + str(tag.get('href')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1amount of URLs has been processed\n",
      "2amount of URLs has been processed\n",
      "3amount of URLs has been processed\n",
      "4amount of URLs has been processed\n",
      "5amount of URLs has been processed\n",
      "6amount of URLs has been processed\n",
      "7amount of URLs has been processed\n",
      "8amount of URLs has been processed\n",
      "9amount of URLs has been processed\n",
      "10amount of URLs has been processed\n",
      "11amount of URLs has been processed\n",
      "12amount of URLs has been processed\n",
      "13amount of URLs has been processed\n",
      "14amount of URLs has been processed\n",
      "15amount of URLs has been processed\n",
      "16amount of URLs has been processed\n",
      "17amount of URLs has been processed\n",
      "18amount of URLs has been processed\n",
      "19amount of URLs has been processed\n",
      "20amount of URLs has been processed\n",
      "21amount of URLs has been processed\n",
      "22amount of URLs has been processed\n",
      "23amount of URLs has been processed\n",
      "24amount of URLs has been processed\n",
      "25amount of URLs has been processed\n",
      "26amount of URLs has been processed\n",
      "27amount of URLs has been processed\n",
      "28amount of URLs has been processed\n",
      "29amount of URLs has been processed\n",
      "30amount of URLs has been processed\n",
      "31amount of URLs has been processed\n",
      "32amount of URLs has been processed\n",
      "33amount of URLs has been processed\n",
      "34amount of URLs has been processed\n",
      "35amount of URLs has been processed\n",
      "36amount of URLs has been processed\n",
      "37amount of URLs has been processed\n",
      "38amount of URLs has been processed\n",
      "39amount of URLs has been processed\n",
      "40amount of URLs has been processed\n",
      "41amount of URLs has been processed\n",
      "42amount of URLs has been processed\n",
      "43amount of URLs has been processed\n",
      "44amount of URLs has been processed\n",
      "45amount of URLs has been processed\n",
      "46amount of URLs has been processed\n",
      "47amount of URLs has been processed\n",
      "48amount of URLs has been processed\n",
      "49amount of URLs has been processed\n",
      "50amount of URLs has been processed\n",
      "51amount of URLs has been processed\n",
      "52amount of URLs has been processed\n",
      "53amount of URLs has been processed\n",
      "54amount of URLs has been processed\n",
      "55amount of URLs has been processed\n",
      "56amount of URLs has been processed\n",
      "57amount of URLs has been processed\n",
      "58amount of URLs has been processed\n",
      "59amount of URLs has been processed\n",
      "60amount of URLs has been processed\n",
      "61amount of URLs has been processed\n",
      "62amount of URLs has been processed\n",
      "63amount of URLs has been processed\n",
      "64amount of URLs has been processed\n",
      "65amount of URLs has been processed\n",
      "66amount of URLs has been processed\n",
      "67amount of URLs has been processed\n",
      "68amount of URLs has been processed\n",
      "69amount of URLs has been processed\n",
      "70amount of URLs has been processed\n",
      "71amount of URLs has been processed\n",
      "72amount of URLs has been processed\n",
      "73amount of URLs has been processed\n",
      "74amount of URLs has been processed\n",
      "75amount of URLs has been processed\n",
      "76amount of URLs has been processed\n",
      "77amount of URLs has been processed\n",
      "78amount of URLs has been processed\n",
      "79amount of URLs has been processed\n",
      "80amount of URLs has been processed\n",
      "81amount of URLs has been processed\n",
      "82amount of URLs has been processed\n",
      "83amount of URLs has been processed\n",
      "84amount of URLs has been processed\n",
      "85amount of URLs has been processed\n",
      "86amount of URLs has been processed\n",
      "87amount of URLs has been processed\n",
      "88amount of URLs has been processed\n",
      "89amount of URLs has been processed\n",
      "90amount of URLs has been processed\n",
      "91amount of URLs has been processed\n",
      "92amount of URLs has been processed\n",
      "93amount of URLs has been processed\n",
      "94amount of URLs has been processed\n",
      "95amount of URLs has been processed\n",
      "96amount of URLs has been processed\n",
      "97amount of URLs has been processed\n"
     ]
    }
   ],
   "source": [
    "errorCount = 1\n",
    "while True:\n",
    "    try:\n",
    "        from io import BytesIO\n",
    "        from urllib.request import urlopen\n",
    "        from zipfile import ZipFile\n",
    "        totalData = pd.DataFrame()\n",
    "        allTheCorrectData = pd.DataFrame()\n",
    "        countOfUrls = 1\n",
    "\n",
    "\n",
    "        master = \"master.zip\"\n",
    "        for url in newNewUrls:\n",
    "            r=requests.get(url)\n",
    "            data=r.text\n",
    "            soup = BeautifulSoup(data, 'lxml')\n",
    "            for tag in soup.find_all('a'):\n",
    "                if tag.text == master:\n",
    "                    zipurl = url + tag.text\n",
    "                    with urlopen(zipurl) as zipresp:\n",
    "                        with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                            zfile.extractall('C:\\\\Users\\\\Cody\\\\Documents\\\\2017-2018 School Year\\\\Semester 2\\\\CIS440\\\\index files')\n",
    "                            zfile.close()\n",
    "            os.chdir('C:\\\\Users\\\\Cody\\\\Documents\\\\2017-2018 School Year\\\\Semester 2\\\\CIS440\\\\index files')\n",
    "            with open('master.idx', 'rb') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            data =data.decode(\"utf-8\", errors='replace')\n",
    "            data = data.split('|')\n",
    "            data = data[1:]\n",
    "\n",
    "            companies = []\n",
    "            forms = []\n",
    "            dateFiled = []\n",
    "            hyperLink = []\n",
    "\n",
    "            count = 0\n",
    "            while count < len(data):\n",
    "                companies.append(data[count])\n",
    "                count+=4\n",
    "\n",
    "            count = 1\n",
    "            while count < len(data):\n",
    "                forms.append(data[count])\n",
    "                count+=4\n",
    "\n",
    "            count = 2\n",
    "            while count < len(data):\n",
    "                dateFiled.append(data[count])\n",
    "                count+=4\n",
    "\n",
    "            count = 3\n",
    "            while count < len(data):\n",
    "                hyperLink.append(data[count])\n",
    "                count+=4\n",
    "\n",
    "            masterData = pd.DataFrame({'Company Names' : companies, 'Forms':forms,'Date Filed':dateFiled,'hyperLink':hyperLink})\n",
    "            masterData = masterData.iloc[1:,]\n",
    "            correctData = masterData[masterData['Forms'] == '10-K']                                          \n",
    "            totalData = totalData.append(masterData)\n",
    "            allTheCorrectData = allTheCorrectData.append(correctData)                 \n",
    "            print(str(countOfUrls) + 'amount of URLs has been processed')\n",
    "            countOfUrls += 1\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "        print(\"There was an error this is number \" + str(errorCount))\n",
    "        errorCount += 1\n",
    "        continue\n",
    "    break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correctList = []\n",
    "for i in allTheCorrectData['hyperLink']:\n",
    "    correctList.append(i.split('\\r')[0])\n",
    "dataUrls = []\n",
    "for i in correctList:\n",
    "    dataUrls.append('https://www.sec.gov/Archives/' + i)\n",
    "    \n",
    "allTheCorrectData['hyperLink'] = dataUrls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = allTheCorrectData[allTheCorrectData['Date Filed'] > '2017-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = ['ESPP','espp','Employee Stock Purchase Plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDALLION FINANCIAL CORP has a keyword which is Employee Stock Purchase Plan\n",
      "HENRY SCHEIN INC has a keyword which is ESPP\n",
      "CORE LABORATORIES N V has a keyword which is Employee Stock Purchase Plan\n",
      "KENTUCKY BANCSHARES INC /KY/ has a keyword which is ESPP\n",
      "IMPAC MORTGAGE HOLDINGS INC has a keyword which is ESPP\n",
      "SCHWEITZER MAUDUIT INTERNATIONAL INC has a keyword which is ESPP\n",
      "BLONDER TONGUE LABORATORIES INC has a keyword which is Employee Stock Purchase Plan\n",
      "NOVAVAX INC has a keyword which is ESPP\n",
      "WATERS CORP /DE/ has a keyword which is ESPP\n",
      "INSPERITY, INC. has a keyword which is Employee Stock Purchase Plan\n",
      "DISH Network CORP has a keyword which is Employee Stock Purchase Plan\n",
      "BROADWAY FINANCIAL CORP \\DE\\ has a keyword which is Employee Stock Purchase Plan\n",
      "TUCSON ELECTRIC POWER CO has a keyword which is Employee Stock Purchase Plan\n",
      "SANGAMO THERAPEUTICS, INC has a keyword which is Employee Stock Purchase Plan\n",
      "TG THERAPEUTICS, INC. has a keyword which is ESPP\n",
      "NORTHWEST PIPE CO has a keyword which is Employee Stock Purchase Plan\n",
      "Acacia Diversified Holdings, Inc. has a keyword which is Employee Stock Purchase Plan\n",
      "TENGASCO INC has a keyword which is Employee Stock Purchase Plan\n",
      "SOUTHERN COPPER CORP/ has a keyword which is Employee Stock Purchase Plan\n",
      "INTEVAC INC has a keyword which is Employee Stock Purchase Plan\n",
      "LEARNING TREE INTERNATIONAL, INC. has a keyword which is Employee Stock Purchase Plan\n",
      "AMEREN CORP has a keyword which is ESPP\n",
      "MMA CAPITAL MANAGEMENT, LLC has a keyword which is Employee Stock Purchase Plan\n",
      "DUKE REALTY LIMITED PARTNERSHIP/ has a keyword which is Employee Stock Purchase Plan\n",
      "AMERICAN EXPRESS CREDIT ACCOUNT MASTER TRUST has a keyword which is Employee Stock Purchase Plan\n",
      "IMPAX LABORATORIES INC has a keyword which is ESPP\n",
      "BCTC IV ASSIGNOR CORP has a keyword which is Employee Stock Purchase Plan\n",
      "TANGER PROPERTIES LTD PARTNERSHIP /NC/ has a keyword which is Employee Stock Purchase Plan\n"
     ]
    }
   ],
   "source": [
    "textList = []\n",
    "count2 = 0\n",
    "count1 = 0\n",
    "index = 0\n",
    "for url in x['hyperLink']:\n",
    "    r=requests.get(url)\n",
    "    data=r.text\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    keywordTF = False\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        for z in keywords:\n",
    "            if z in paragraph.text:\n",
    "                textList.append(paragraph.text)\n",
    "                keywordTF = True\n",
    "                if keywordTF == False:\n",
    "                    count2 +=1\n",
    "                    index +=1\n",
    "                else:\n",
    "                    count1 +=1\n",
    "                    print(x['Company Names'].iloc[index] + ' has a keyword which is ' + z)\n",
    "                    index +=1\n",
    "    \n",
    "    \n",
    "print(str(count2) + ' out of ' + str(len(x['hyperLink'])) + ' dont have a keyword')\n",
    "print(str(count1) + ' have keywords out of ' + str(len(x['hyperLink'])) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 out of 21 dont have a keyword\n",
      "8 have keywords out of 21\n"
     ]
    }
   ],
   "source": [
    "# textList = []\n",
    "# count = 0\n",
    "# count1 = 0\n",
    "# for i in documents:\n",
    "#     keywordTF = False\n",
    "#     for paragraph in i.find_all('p'):\n",
    "#         for z in keywords:\n",
    "#             if z in paragraph.text:\n",
    "#                 textList.append(paragraph.text)\n",
    "#                 keywordTF = True\n",
    "#     if keywordTF == False:\n",
    "#         count +=1\n",
    "#     else:\n",
    "#         count1 +=1\n",
    "\n",
    "# print(str(count) + ' out of ' + str(len(documents)) + ' dont have a keyword')\n",
    "# print(str(count1) + ' have keywords out of ' + str(len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'Employee Stock Purchase Plan' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-58a65ff03dc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0myour_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlist_of_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myour_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Employee Stock Purchase Plan'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnext_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_of_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist_of_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Employee Stock Purchase Plan'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: 'Employee Stock Purchase Plan' is not in list"
     ]
    }
   ],
   "source": [
    "your_string = textList[0]\n",
    "list_of_words = your_string.split('Employee Stock Purchase Plan')\n",
    "next_word = list_of_words[list_of_words.index('Employee Stock Purchase Plan') + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textList = []\n",
    "count = 0\n",
    "count1 = 0\n",
    "for i in documents[2]:\n",
    "    keywordTF = False\n",
    "    for paragraph in i.find_all('p'):\n",
    "        for z in keywords:\n",
    "            if z in paragraph.text:\n",
    "                textList.append(paragraph.text)\n",
    "                keywordTF = True\n",
    "    if keywordTF == False:\n",
    "        count +=1\n",
    "    else:\n",
    "        count1 +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CODE IS DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textLis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-4e477b3a8e0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextLis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'textLis' is not defined"
     ]
    }
   ],
   "source": [
    "len(textLis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# listOf10ks = []\n",
    "# actual10Ks = []\n",
    "# linkTo10K = ''\n",
    "# mainurl = \"https://www.sec.gov\"\n",
    "# count = 1\n",
    "\n",
    "# cikList = ['0001466593', '0001121484']\n",
    "# for cik in cikList:\n",
    "#     url = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=\" + cik + \"&owner=include&count=40\"\n",
    "#     r=requests.get(url)\n",
    "#     data=r.text\n",
    "#     soup = BeautifulSoup(data, 'lxml')\n",
    "#     for links in soup.find_all('tr'):\n",
    "#         if '10-K' in links.text:\n",
    "#             for element in links.find_all('a'):\n",
    "#                 if element.text == '\\xa0Documents':\n",
    "#                     listOf10ks = element.get('href')\n",
    "#                     r=requests.get(mainurl + listOf10ks)\n",
    "#                     data=r.text\n",
    "#                     soup = BeautifulSoup(data, 'lxml')\n",
    "#                     for links in soup.find_all('tr'):\n",
    "#                         if '10-K' in links.text:\n",
    "#                             for element in links.find_all('a'):\n",
    "#                                 print(element.text)\n",
    "#                                 linkTo10K = element.get('href')\n",
    "#                                 r=requests.get(mainurl + linkTo10K)\n",
    "#                                 data=r.text\n",
    "#                                 soup = BeautifulSoup(data, 'lxml')\n",
    "#                                 actual10Ks.append(soup)\n",
    "#                                 print(str(count) + ' 10K is appended')\n",
    "#                                 count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     for links in soup.find_all('tr'):\n",
    "#         if '10-K' in links.text:\n",
    "#             for element in links.find_all('a'):\n",
    "#                 if element.text == '\\xa0Documents':\n",
    "#                     listOf10ks = element.get('href')\n",
    "#                     r=requests.get(mainurl + listOf10ks)\n",
    "#                     data=r.text\n",
    "#                     soup = BeautifulSoup(data, 'lxml')\n",
    "#                     for links in soup.find_all('tr'):\n",
    "#                         if '10-K' in links.text:\n",
    "#                             for element in links.find_all('a'):\n",
    "#                                 print(element.text)\n",
    "#                                 linkTo10K = element.get('href')\n",
    "#                                 r=requests.get(mainurl + linkTo10K)\n",
    "#                                 data=r.text\n",
    "#                                 soup = BeautifulSoup(data, 'lxml')\n",
    "#                                 actual10Ks.append(soup)\n",
    "#                                 print(str(count) + ' 10K is appended')\n",
    "#                                 count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.chdir('C:\\\\Users\\\\Cody\\\\Documents\\\\2017-2018 School Year\\\\Semester 2\\\\CIS440')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('master.idx', 'rb') as f:\n",
    "#     data = f.read()\n",
    "\n",
    "# data =data.decode(\"utf-8\")\n",
    "# data = data.split('|')\n",
    "# data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# companies = []\n",
    "# forms = []\n",
    "# dateFiled = []\n",
    "# hyperLink = []\n",
    "# totalData = pd.DataFrame()\n",
    "# allTheCorrectData = pd.DataFrame()\n",
    "\n",
    "\n",
    "# data =data.decode(\"utf-8\")\n",
    "# data = data.split('|')\n",
    "# data = data[1:]\n",
    "\n",
    "# count = 0\n",
    "# while count < len(data):\n",
    "#     companies.append(data[count])\n",
    "#     count+=4\n",
    "    \n",
    "# count = 1\n",
    "# while count < len(data):\n",
    "#     forms.append(data[count])\n",
    "#     count+=4\n",
    "    \n",
    "# count = 2\n",
    "# while count < len(data):\n",
    "#     dateFiled.append(data[count])\n",
    "#     count+=4\n",
    "    \n",
    "# count = 3\n",
    "# while count < len(data):\n",
    "#     hyperLink.append(data[count])\n",
    "#     count+=4\n",
    "    \n",
    "# masterData = pd.DataFrame({'Company Names' : companies, 'Forms':forms,'Date Filed':dateFiled,'hyperLink':hyperLink})\n",
    "# masterData = masterData.iloc[1:,]\n",
    "# correctData = masterData[masterData['Forms'] == '10-K']\n",
    "\n",
    "# totalData = totalData.append(masterData)\n",
    "# allTheCorrectData = allTheCorrectData.append(correctData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataUrls = []\n",
    "#                     for i in correctList:\n",
    "#                         dataUrls.append('https://www.sec.gov/Archives/' + i)\n",
    "                    \n",
    "#                     documents = []\n",
    "#                     count = 1\n",
    "#                     for url in dataUrls:\n",
    "#                         r=requests.get(url)\n",
    "#                         data=r.text\n",
    "#                         soup = BeautifulSoup(data, 'lxml')\n",
    "#                         documents.append(soup)\n",
    "#                         print('document' + str(count) + 'appended')\n",
    "#                         count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from io import BytesIO\n",
    "# from urllib.request import urlopen\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "# master = \"master.zip\"\n",
    "# r=requests.get(newNewUrls[95])\n",
    "# data=r.text\n",
    "# soup = BeautifulSoup(data, 'lxml')\n",
    "# for tag in soup.find_all('a'):\n",
    "#     if tag.text == master:\n",
    "#         zipurl = newNewUrls[95] + tag.text\n",
    "#         with urlopen(zipurl) as zipresp:\n",
    "#             with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "#                 zfile.extractall('C:\\\\Users\\\\Cody\\\\Documents\\\\2017-2018 School Year\\\\Semester 2\\\\CIS440\\\\index files')\n",
    "#                 zfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
